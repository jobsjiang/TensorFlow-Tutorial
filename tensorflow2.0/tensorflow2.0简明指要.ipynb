{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/70232196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Eager执行，当你执行某个操作时是立即返回结果的\n",
    "* eager模式下如何计算梯度\n",
    "* 对于eager执行，每个tape会记录当前所执行的操作，这个tape只对当前计算有效，并计算相应的梯度。PyTorch也是动态图模式，但是与TensorFlow不同，它是每个需要计算Tensor会拥有grad_fn以追踪历史操作的梯度。\n",
    "* tensorflow2.0引入了tf.function和AutoGraph来缩小eager执行和Graph模式的性能差距，其核心是将一系列的python语法转化为高性能的graph操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AutoGraph在TensorFlow 1.x已经推出，主要是可以将一些常用的Python代码转化为TensorFlow支持的Graph代码。\n",
    "* Graph模式下的执行效率是最高的，原来的代码在eager模式下效率次之，经AutoGraph转换后的代码效率最低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 性能优化:tf.function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf.function可以将一个func中的TensorFlow操作构建为一个Graph，这样在调用时是执行这个Graph，这样计算性能更优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型构建:tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TensorFlow 1.x存在tf.layers以及tf.contrib.slim等高级API来创建模型，但是2.0仅仅支持tf.keras.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TensorFlow 2.0的数据加载还是采用tf.data，不过在eager模式下，tf.data.Dataset这个类将成为一个Python迭代器，我们可以直接取值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 2us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "13565952/26421880 [==============>...............] - ETA: 10s"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images,\n",
    "                               test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Adding a dimension to the array -> new shape == (28, 28, 1)\n",
    "train_images = train_images[..., None]\n",
    "test_images = test_images[..., None]\n",
    "\n",
    "# Getting the images in [0, 1] range.\n",
    "train_images = train_images / np.float32(255)\n",
    "test_images = test_images / np.float32(255)\n",
    "\n",
    "train_labels = train_labels.astype('int64')\n",
    "test_labels = test_labels.astype('int64')\n",
    "\n",
    "# dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_images, train_labels)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_images, test_labels)).batch(32)\n",
    "\n",
    "# Model\n",
    "\n",
    "\n",
    "class MyModel(tf.keras.Sequential):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__([\n",
    "            tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation=None)\n",
    "        ])\n",
    "\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "# optimizer\n",
    "initial_learning_rate = 1e-4\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
    "\n",
    "# checkpoint\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    step=tf.Variable(0), optimizer=optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(checkpoint, './tf_ckpts', max_to_keep=3)\n",
    "\n",
    "# loss function\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# metric\n",
    "train_loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "test_loss_metric = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='test_accuracy')\n",
    "\n",
    "# define a train step\n",
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_object(targets, predictions)\n",
    "        loss += sum(model.losses)  # add other losses\n",
    "    # compute gradients and update variables\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss_metric(loss)\n",
    "    train_acc_metric(targets, predictions)\n",
    "\n",
    "# define a test step\n",
    "@tf.function\n",
    "def test_step(inputs, targets):\n",
    "    predictions = model(inputs, training=False)\n",
    "    loss = loss_object(targets, predictions)\n",
    "    test_loss_metric(loss)\n",
    "    test_acc_metric(targets, predictions)\n",
    "\n",
    "\n",
    "# train loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    # Iterate over the batches of the dataset\n",
    "    for step, (inputs, targets) in enumerate(train_ds):\n",
    "        train_step(inputs, targets)\n",
    "        checkpoint.step.assign_add(1)\n",
    "        # log every 20 step\n",
    "        if step % 20 == 0:\n",
    "            manager.save()  # save checkpoint\n",
    "            print('Epoch: {}, Step: {}, Train Loss: {}, Train Accuracy: {}'.format(\n",
    "                epoch, step, train_loss_metric.result().numpy(),\n",
    "                train_acc_metric.result().numpy())\n",
    "            )\n",
    "            train_loss_metric.reset_states()\n",
    "            train_acc_metric.reset_states()\n",
    "\n",
    "# do test\n",
    "for inputs, targets in test_ds:\n",
    "    test_step(inputs, targets)\n",
    "print('Test Loss: {}, Test Accuracy: {}'.format(\n",
    "    test_loss_metric.result().numpy(),\n",
    "    test_acc_metric.result().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
