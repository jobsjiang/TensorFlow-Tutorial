{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/FrancescoSaverioZuppichini/Tensorflow-Dataset-Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jianghaitao1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24436941 0.90492547]\n"
     ]
    }
   ],
   "source": [
    "# 载入数据 \n",
    "# 情况一\n",
    "x = np.random.sample((100,2))\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()  # one shot迭代器\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.23430618, 0.50178319]), array([0.92955796]))\n"
     ]
    }
   ],
   "source": [
    "# 情况二\n",
    "features, labels = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9910476  0.07231998]\n"
     ]
    }
   ],
   "source": [
    "# 情况三\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([100, 2]))\n",
    "\n",
    "iter = dataset.make_initializable_iterator()  # 可初始化的迭代器,使用创建一个动态的数据集\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iter.initializer)\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23532043 0.94944435]\n"
     ]
    }
   ],
   "source": [
    "# 情况四\n",
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "data = np.random.sample((100,2))\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iter.initializer, feed_dict={ x: data })\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n",
      "[[2]\n",
      " [3]]\n",
      "[[3]\n",
      " [4]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "# 情况五\n",
    "# 从生成器导入\n",
    "sequence = np.array([[[1]],[[2],[3]],[[3],[4],[5]]])\n",
    "\n",
    "def generator():\n",
    "    for el in sequence:\n",
    "        yield el\n",
    "\n",
    "dataset = tf.data.Dataset().batch(1).from_generator(generator,\n",
    "                                           output_types= tf.int64, \n",
    "                                           output_shapes=(tf.TensorShape([None, 1])))\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iter.initializer)\n",
    "    print(sess.run(el))\n",
    "    print(sess.run(el))\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 2.], dtype=float32), array([0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# initializable iterator to switch between data  初始化迭代器转换数据\n",
    "EPOCHS = 10\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.array([[1,2]]), np.array([[0]]))\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "features, labels = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     initialise iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1]})\n",
    "    for _ in range(EPOCHS):\n",
    "        sess.run([features, labels])\n",
    "#     switch to test data\n",
    "    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1]})\n",
    "    print(sess.run([features, labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.77820065, 0.81910271]), array([0.03881743])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reinitializable iterator to switch between Datasets\n",
    "EPOCHS = 10\n",
    "# making fake data using numpy\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.random.sample((10,2)), np.random.sample((10,1)))\n",
    "# create two datasets, one for training and one for test\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_data)\n",
    "# create a iterator of the correct shape and type\n",
    "iter = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes) # 可重新初始化的迭代器\n",
    "features, labels = iter.get_next()\n",
    "# create the initialisation operations\n",
    "train_init_op = iter.make_initializer(train_dataset)\n",
    "test_init_op = iter.make_initializer(test_dataset)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(train_init_op) # switch to train dataset\n",
    "    for _ in range(EPOCHS):\n",
    "        sess.run([features, labels])\n",
    "    sess.run(test_init_op) # switch to val dataset\n",
    "    print(sess.run([features, labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6625369  0.22438107] [0.7979086]\n",
      "[0.13105573 0.46070796] [0.9172081]\n",
      "[0.71677524 0.5331036 ] [0.7364363]\n",
      "[0.7112605 0.5985909] [0.8797758]\n",
      "[0.5903894  0.24668247] [0.5267125]\n",
      "[0.6090924  0.14256322] [0.73156434]\n",
      "[0.30085415 0.696469  ] [0.7839038]\n",
      "[0.54066765 0.7401784 ] [0.9504862]\n",
      "[0.6393063 0.786051 ] [0.76454973]\n",
      "[0.04757722 0.81057656] [0.36786303]\n",
      "[0.2886261 0.5042412] [0.52233875]\n"
     ]
    }
   ],
   "source": [
    "# feedable iterator to switch between iterators # 可馈送的迭代器\n",
    "EPOCHS = 10 \n",
    "# making fake data using numpy\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.random.sample((10,2)), np.random.sample((10,1)))\n",
    "# create placeholder\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "# create two datasets, one for training and one for test\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "# create the iterators from the dataset\n",
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "test_iterator = test_dataset.make_initializable_iterator()\n",
    "# same as in the doc https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iter = tf.data.Iterator.from_string_handle(\n",
    "    handle, train_dataset.output_types, train_dataset.output_shapes)\n",
    "next_elements = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    test_handle = sess.run(test_iterator.string_handle())\n",
    "    \n",
    "    # initialise iterators. In our case we could have used the 'one-shot' iterator instead,\n",
    "    # and directly feed the data insted the Dataset.from_tensor_slices function, but this\n",
    "    # approach is more general\n",
    "    sess.run(train_iterator.initializer, feed_dict={ x: train_data[0], y: train_data[1]})\n",
    "    sess.run(test_iterator.initializer, feed_dict={ x: test_data[0], y: test_data[1]})\n",
    "    \n",
    "    for _ in range(EPOCHS):\n",
    "        x,y = sess.run(next_elements, feed_dict = {handle: train_handle})\n",
    "        print(x, y)\n",
    "        \n",
    "    x,y = sess.run(next_elements, feed_dict = {handle: test_handle})\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.78797767 0.55979762]\n",
      " [0.6423894  0.2652314 ]\n",
      " [0.20721323 0.0302527 ]\n",
      " [0.42905493 0.25536461]]\n"
     ]
    }
   ],
   "source": [
    "# BATCHING 数据分批\n",
    "BATCH_SIZE = 4\n",
    "x = np.random.sample((100,2))\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n"
     ]
    }
   ],
   "source": [
    "# REPEAT  重复取数据  repeat可以指定数据集被迭代的次数，如果不传输任何参数，循环将永久进行\n",
    "BATCH_SIZE = 4 \n",
    "x = np.array([[1],[2],[3],[4]])\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset = dataset.repeat()\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for _ in range(8):\n",
    "        print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[4]\n",
      "[6]\n",
      "[8]\n"
     ]
    }
   ],
   "source": [
    "# MAP 对数据集中的所有成员应用定制化函数\n",
    "x = np.array([[1],[2],[3],[4]])\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset = dataset.map(lambda x: x*2)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     this will run forever\n",
    "        for _ in range(len(x)):\n",
    "            print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "# SHUFFLE 默认是在每一个epoch中将数据集shuffle一次，数据集shuffle是避免过拟合的重要方法\n",
    "BATCH_SIZE = 4\n",
    "x = np.array([[1],[2],[3],[4]])\n",
    "# make a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset = dataset.shuffle(buffer_size=100)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "el = iter.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.1828\n",
      "Iter: 1, Loss: 0.1729\n",
      "Iter: 2, Loss: 0.1637\n",
      "Iter: 3, Loss: 0.1550\n",
      "Iter: 4, Loss: 0.1470\n",
      "Iter: 5, Loss: 0.1396\n",
      "Iter: 6, Loss: 0.1328\n",
      "Iter: 7, Loss: 0.1266\n",
      "Iter: 8, Loss: 0.1209\n",
      "Iter: 9, Loss: 0.1158\n"
     ]
    }
   ],
   "source": [
    "# how to pass the value to a model  如何将值传递给模型\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "# using two numpy arrays\n",
    "features, labels = (np.array([np.random.sample((100,2))]), \n",
    "                    np.array([np.random.sample((100,1))]))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)\n",
    "\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "x, y = iter.get_next()\n",
    "\n",
    "# make a simple model\n",
    "net = tf.layers.dense(x, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
    "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(EPOCHS):\n",
    "        _, loss_value = sess.run([train_op, loss])\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Iter: 0, Loss: 0.1560\n",
      "Iter: 1, Loss: 0.1146\n",
      "Iter: 2, Loss: 0.0975\n",
      "Iter: 3, Loss: 0.0897\n",
      "Iter: 4, Loss: 0.1233\n",
      "Iter: 5, Loss: 0.0861\n",
      "Iter: 6, Loss: 0.0836\n",
      "Iter: 7, Loss: 0.0816\n",
      "Iter: 8, Loss: 0.1172\n",
      "Iter: 9, Loss: 0.0812\n",
      "Test Loss: 0.101598\n"
     ]
    }
   ],
   "source": [
    "# Wrapping all together -> Switch between train and test set using Initializable iterator\n",
    "# 使用可初始化迭代器在训练集和测试集之间转换\n",
    "EPOCHS = 10\n",
    "# create a placeholder to dynamically switch between batch sizes\n",
    "batch_size = tf.placeholder(tf.int64)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size).repeat()\n",
    "\n",
    "# using two numpy arrays\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.random.sample((20,2)), np.random.sample((20,1)))\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "features, labels = iter.get_next()\n",
    "# make a simple model\n",
    "net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
    "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "n_batches = train_data[0].shape[0] // BATCH_SIZE\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # initialise iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1], batch_size: BATCH_SIZE})\n",
    "    print('Training...')\n",
    "    for i in range(EPOCHS):\n",
    "        tot_loss = 0\n",
    "        for _ in range(n_batches):\n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            tot_loss += loss_value\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / n_batches))\n",
    "    # initialise iterator with test data\n",
    "    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1], batch_size: test_data[0].shape[0]})\n",
    "    print('Test Loss: {:4f}'.format(sess.run(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Iter: 0, Loss: 0.2700\n",
      "Iter: 1, Loss: 0.2559\n",
      "Iter: 2, Loss: 0.2499\n",
      "Iter: 3, Loss: 0.1920\n",
      "Iter: 4, Loss: 0.1953\n",
      "Iter: 5, Loss: 0.1434\n",
      "Iter: 6, Loss: 0.1649\n",
      "Iter: 7, Loss: 0.1048\n",
      "Iter: 8, Loss: 0.1053\n",
      "Iter: 9, Loss: 0.1120\n",
      "Test Loss: 0.111329\n"
     ]
    }
   ],
   "source": [
    "# Wrapping all together -> Switch between train and test set using Reinitializable iterator\n",
    "EPOCHS = 10\n",
    "# create a placeholder to dynamically switch between batch sizes\n",
    "batch_size = tf.placeholder(tf.int64)\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size).repeat()\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size) # always batch even if you want to one shot it\n",
    "# using two numpy arrays\n",
    "train_data = (np.random.sample((100,2)), np.random.sample((100,1)))\n",
    "test_data = (np.random.sample((20,2)), np.random.sample((20,1)))\n",
    "\n",
    "# create a iterator of the correct shape and type\n",
    "iter = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "features, labels = iter.get_next()\n",
    "# create the initialisation operations\n",
    "train_init_op = iter.make_initializer(train_dataset)\n",
    "test_init_op = iter.make_initializer(test_dataset)\n",
    "\n",
    "# make a simple model\n",
    "net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input\n",
    "net = tf.layers.dense(net, 8, activation=tf.tanh)\n",
    "prediction = tf.layers.dense(net, 1, activation=tf.tanh)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # initialise iterator with train data\n",
    "    sess.run(train_init_op, feed_dict = {x : train_data[0], y: train_data[1], batch_size: 16})\n",
    "    print('Training...')\n",
    "    for i in range(EPOCHS):\n",
    "        tot_loss = 0\n",
    "        for _ in range(n_batches):\n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            tot_loss += loss_value\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / n_batches))\n",
    "    # initialise iterator with test data\n",
    "    sess.run(test_init_op, feed_dict = {x : test_data[0], y: test_data[1], batch_size:len(test_data[0])})\n",
    "    print('Test Loss: {:4f}'.format(sess.run(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-513883c32b98>:3: make_csv_dataset (from tensorflow.contrib.data.python.ops.readers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.make_csv_dataset(...)`.\n",
      "OrderedDict([('sentiment', <tf.Tensor 'IteratorGetNext_15:0' shape=(32,) dtype=int32>), ('text', <tf.Tensor 'IteratorGetNext_15:1' shape=(32,) dtype=string>)])\n",
      "[array([b'going to go take a shower then go to \"bed\", i graduate tomorrow xd yay!! gonna miss my old school so many great memories',\n",
      "       b'@MENTION aww gladd youu won it! , your soo funny!',\n",
      "       b\"strange ! i've just waken up at midnight. n the best part, i really couldn't sleep again\",\n",
      "       b'my brother is in his underwear, playing guitar on my floor.. lawl',\n",
      "       b'weather looks nasty', b'@MENTION when do i get it?',\n",
      "       b\"totally agree with him on that one!nighty,night people!don't forget to say your prayers\",\n",
      "       b'@MENTION haha, that it does', b'@MENTION join berrytastic.com',\n",
      "       b'@MENTION hey man, sorry about this horrible shitty phoneless time! 2pm okay? got to get up early and do reviewing',\n",
      "       b'@MENTION i love you too jenna! so much.',\n",
      "       b'@MENTION hope you have your waterproofs fella i understand we are all going to get wet today',\n",
      "       b'@MENTION maine kaha morning',\n",
      "       b\"wifi for my phone is kinda on and off. i definitely think its damaged. i couldn't update twitter for like 10 hours i just woke up btw.\",\n",
      "       b'r @MENTION: filthy and demeaning. but for which of the two?',\n",
      "       b\"says let me tell you, what's hurting me\",\n",
      "       b'just heard about bob bogle, rip',\n",
      "       b\"@MENTION this is a two-handed tractor/mower. for the sake of all living creatures i'm not going to tell him you said that\",\n",
      "       b'i would like to thank everybody for the follow fridays! you rock!',\n",
      "       b'these have been very long 8 hour days.',\n",
      "       b'@MENTION god is great',\n",
      "       b'@MENTION maybe next time congrats @MENTION',\n",
      "       b'im a premium member of picnik now and couldnt resist being a member of flickr premium too. still satisfied',\n",
      "       b'has drop off @MENTION already now miss her already',\n",
      "       b\"@MENTION ditto to mallverine's original post!\",\n",
      "       b\"brokeagain hillary's just fitted wooden venetions in the lounge. another debit card bashing...thx wife!\",\n",
      "       b'my biggest wish is, that i meeet you. but i\\xc2\\xac\\xc2\\xa5m so sad, because i nerver meet you',\n",
      "       b'not impressed with how baker and taylor address new opportunities. almost confrontational.',\n",
      "       b'hating my computer cause when i change my dumb dp it puts an x over it',\n",
      "       b'2/3 of save the date cards assembled. hotel info cards printed and cut. need to get more glue and stamp ink pad tomorrow',\n",
      "       b'@MENTION hahaha thats true! but you can trust that i am tweeting myself lol',\n",
      "       b'@MENTION why limit your story to 140 ch. tell all what is annoying you at ,it will help relieve your stress'],\n",
      "      dtype=object), array([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
      "       1, 0, 1, 0, 0, 0, 0, 1, 1, 1])]\n"
     ]
    }
   ],
   "source": [
    "# load a csv\n",
    "CSV_PATH = '../../dataset/tweets.csv'\n",
    "dataset = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=32)\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "next = iter.get_next()\n",
    "print(next) # next is a dict with key=columns names and value=column data\n",
    "inputs, labels = next['text'], next['sentiment']\n",
    "\n",
    "with  tf.Session() as sess:\n",
    "    print(sess.run([inputs,labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_time = {}\n",
    "# copied form https://medium.com/pythonhive/python-decorator-to-measure-the-execution-time-of-methods-fa04cb6bb36d\n",
    "def how_much(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        \n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__)\n",
    "            kw['log_time'][name] = (te - ts)\n",
    "            \n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((5000, 32, 32), (5000, 20)) ((1000, 32, 32), (1000, 20))\n"
     ]
    }
   ],
   "source": [
    "# benchmark\n",
    "import time\n",
    "DATA_SIZE = 5000\n",
    "DATA_SHAPE = ((32,32),(20,))\n",
    "BATCH_SIZE = 64 \n",
    "N_BATCHES = DATA_SIZE // BATCH_SIZE\n",
    "EPOCHS = 10\n",
    "\n",
    "test_size = (DATA_SIZE//100)*20 \n",
    "\n",
    "train_shape = ((DATA_SIZE, *DATA_SHAPE[0]),(DATA_SIZE, *DATA_SHAPE[1]))\n",
    "test_shape = ((test_size, *DATA_SHAPE[0]),(test_size, *DATA_SHAPE[1]))\n",
    "print(train_shape, test_shape)\n",
    "train_data = (np.random.sample(train_shape[0]), np.random.sample(train_shape[1]))\n",
    "test_data = (np.random.sample(test_shape[0]), np.random.sample(test_shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 32, 32] [None, 20]\n",
      "one_shot\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "initialisable\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "reinitializable\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "feedable\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1.52913498878479, 'initialisable'),\n",
       " (1.7129340171813965, 'reinitializable'),\n",
       " (2.079695701599121, 'feedable'),\n",
       " (5.222063779830933, 'one_shot')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# used to keep track of the methodds\n",
    "log_time = {}\n",
    "\n",
    "tf.reset_default_graph()\n",
    "# sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "# sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\n",
    "# sess = tf.InteractiveSession()\n",
    "# sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\n",
    "sess = tf.Session()\n",
    "input_shape = [None, *DATA_SHAPE[0]]  # [None, 64, 64, 3]\n",
    "output_shape = [None, *DATA_SHAPE[1]]  # [None, 20]\n",
    "print(input_shape, output_shape)\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=input_shape), tf.placeholder(\n",
    "    tf.float32, shape=output_shape)\n",
    "\n",
    "\n",
    "@how_much\n",
    "def one_shot(**kwargs):\n",
    "    print('one_shot')\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        train_data).batch(BATCH_SIZE).repeat()\n",
    "    train_el = train_dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        test_data).batch(BATCH_SIZE).repeat()\n",
    "    test_el = test_dataset.make_one_shot_iterator().get_next()\n",
    "    for i in range(EPOCHS):\n",
    "        print(i)\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(train_el)\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(test_el)\n",
    "\n",
    "\n",
    "@how_much\n",
    "def initialisable(**kwargs):\n",
    "    print('initialisable')\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (x, y)).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "    iter = dataset.make_initializable_iterator()\n",
    "    elements = iter.get_next()\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        print(i)\n",
    "        sess.run(iter.initializer, feed_dict={\n",
    "                 x: train_data[0], y: train_data[1]})\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(elements)\n",
    "        sess.run(iter.initializer, feed_dict={\n",
    "                 x: test_data[0], y: test_data[1]})\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(elements)\n",
    "\n",
    "\n",
    "@how_much\n",
    "def reinitializable(**kwargs):\n",
    "    print('reinitializable')\n",
    "    # create two datasets, one for training and one for test\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (x, y)).batch(BATCH_SIZE).repeat()\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (x, y)).batch(BATCH_SIZE).repeat()\n",
    "    # create a iterator of the correct shape and type\n",
    "    iter = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
    "                                           train_dataset.output_shapes)\n",
    "    elements = iter.get_next()\n",
    "    # create the initialisation operations\n",
    "    train_init_op = iter.make_initializer(train_dataset)\n",
    "    test_init_op = iter.make_initializer(test_dataset)\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        print(i)\n",
    "        sess.run(train_init_op, feed_dict={x: train_data[0], y: train_data[1]})\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(elements)\n",
    "        sess.run(test_init_op, feed_dict={x: test_data[0], y: test_data[1]})\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(elements)\n",
    "\n",
    "\n",
    "@how_much\n",
    "def feedable(**kwargs):\n",
    "    print('feedable')\n",
    "    # create two datasets, one for training and one for test\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (x, y)).batch(BATCH_SIZE).repeat()\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (x, y)).batch(BATCH_SIZE).repeat()\n",
    "    # create the iterators from the dataset\n",
    "    train_iterator = train_dataset.make_initializable_iterator()\n",
    "    test_iterator = test_dataset.make_initializable_iterator()\n",
    "\n",
    "    handle = tf.placeholder(tf.string, shape=[])\n",
    "    iter = tf.data.Iterator.from_string_handle(\n",
    "        handle, train_dataset.output_types, train_dataset.output_shapes)\n",
    "    elements = iter.get_next()\n",
    "\n",
    "    train_handle = sess.run(train_iterator.string_handle())\n",
    "    test_handle = sess.run(test_iterator.string_handle())\n",
    "\n",
    "    sess.run(train_iterator.initializer, feed_dict={x: train_data[0], y: train_data[1]})\n",
    "    sess.run(test_iterator.initializer, feed_dict={x: test_data[0], y: test_data[1]})\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        print(i)\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(elements, feed_dict={handle: train_handle})\n",
    "        for _ in range(N_BATCHES):\n",
    "            sess.run(elements, feed_dict={handle: test_handle})\n",
    "\n",
    "\n",
    "one_shot(log_time=log_time)\n",
    "initialisable(log_time=log_time)\n",
    "reinitializable(log_time=log_time)\n",
    "feedable(log_time=log_time)\n",
    "\n",
    "sorted((value, key) for (key, value) in log_time.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
